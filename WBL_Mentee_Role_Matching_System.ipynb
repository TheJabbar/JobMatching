{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Job Matching WBL for Mentee-Mentor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description :**\n",
    "Merekomendasi role/posisi paling cocok untuk peserta magang (mentee) dalam program WBL (Work-Based Learning) berbasis data profil mentee, sehingga bisa dicocokkan secara tepat dengan kebutuhan mentor atau supervisor.\n",
    "\n",
    "**Actors**\n",
    "1. Supervisor (Mentor)\n",
    "2. Mentee\n",
    "\n",
    "**Main Flow**\n",
    "1. Mentee memasukkan data profil dan skill, mentor memasukkan job request  \n",
    "2. Sistem memproses data mentee dan mencocokkan dengan role yang tersedia  \n",
    "3. Sistem menampilkan rekomendasi role yang paling cocok  \n",
    "4. Supervisor dapat melihat hasil rekomendasi\n",
    "\n",
    "\n",
    "**Alternative Flow :**\n",
    "Jika tidak ditemukan role yang cocok secara otomatis, sistem menyediakan opsi pencarian manual agar mentor tetap dapat memilih mentee sesuai role yang dibutuhkan.\n",
    "\n",
    "**Expected Outcome :**\n",
    "Mentor dapat dengan mudah menyeleksi mentee berdasarkan rekomendasi yang sesuai dengan kebutuhan skill pada posisi yang ditawarkan, sehingga proses seleksi menjadi lebih efisien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting each json into csv\n",
    "\n",
    "files = [\n",
    "    \"*/dataset/allReqParticipant.json\",\n",
    "    \"*/dataset/groupByReserve.json\",\n",
    "    \"*/dataset/groupBySpv.json\",\n",
    "]\n",
    "\n",
    "output_loc = \"*/transformed_src/\"\n",
    "\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "        output_csv = os.path.join(output_loc, os.path.basename(file).replace(\".json\", \".csv\"))\n",
    "        df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allReqParticipant = pd.read_csv(output_loc + \"allReqParticipant.csv\")\n",
    "df_groupByReserve = pd.read_csv(output_loc + \"groupByReserve.csv\")\n",
    "df_groupBySpv = pd.read_csv(output_loc + \"groupBySpv.csv\")\n",
    "\n",
    "list = [df_allReqParticipant, df_groupByReserve, df_groupBySpv]\n",
    "\n",
    "for i in list:\n",
    "    print(f\"{i.info()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupByReserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allReqParticipant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupBySpv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract role data\n",
    "\n",
    "def normalize_nested_column(df, column_name, explode_column=True):\n",
    "    # Handle missing values\n",
    "    df[column_name] = df[column_name].fillna('{}').apply(ast.literal_eval)\n",
    "    \n",
    "    if explode_column:\n",
    "        df = df.explode(column_name).reset_index(drop=True)\n",
    "    \n",
    "    normalized = pd.json_normalize(df[column_name])\n",
    "    df = pd.concat([df.drop(column_name, axis=1), normalized], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req = pd.read_csv('*/transformed_src/allReqParticipant.csv', parse_dates=['createdAt', 'updatedAt'])\n",
    "df_req = normalize_nested_column(df_req, 'roleRequest')\n",
    "\n",
    "df_reserve = pd.read_csv('*/transformed_src/groupByReserve.csv')\n",
    "df_reserve = normalize_nested_column(df_reserve, 'participants')\n",
    "\n",
    "df_spv = pd.read_csv('*/transformed_src/groupBySpv.csv')\n",
    "df_spv = normalize_nested_column(df_spv, 'participants')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_reserve = ['reserve_id', 'reserve_name', 'participant_id', 'participant_name']\n",
    "col_names = ['spv_id', 'spv_name', 'participant_id', 'participant_name']\n",
    "\n",
    "for i in range(4):\n",
    "    df_reserve.columns.values[i] = col_names_reserve[i]\n",
    "    df_spv.columns.values[i] = col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_req.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req['jobDesc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_new = df_req.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df_req_new.columns.tolist()\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "'status', 'projectName', 'projectLocation', 'projectDesc', 'projectLink', 'batchType', 'batchTypeName', 'batchRequestQuota', 'amount', 'createdAt', 'updatedAt', 'internshipPeriodMonth', 'internshipPeriodMonthStart', 'internshipPeriodMonthEnd', 'onJobScheme', 'universityRequest', 'isUniversityRequest', 'internshipPeriodStartDate', 'internshipPeriodEndDate',]\n",
    "\n",
    "df_req_new.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_new.rename(columns={'supervisorName': 'spv_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_new.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_req.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reserve.to_csv('*/transformed_src/df_reserve.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsv = pd.read_csv('*/transformed_src/df_reserve.csv')\n",
    "df_rsv = normalize_nested_column(df_rsv, 'curriculumVitae.educationalBackgrounds')\n",
    "df_rsv = normalize_nested_column(df_rsv, 'curriculumVitae.workExperiences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df_rsv.columns.tolist()\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'curriculumVitae.englishSkill.certificationType', 'curriculumVitae.englishSkill.certificationPublisher', 'curriculumVitae.englishSkill.certificateNumber', 'curriculumVitae.englishSkill.publishDate', 'curriculumVitae.skill.hardSkillDesc', 'curriculumVitae.englishSkill.expireDate', 'curriculumVitae.englishSkill.score', 'curriculumVitae.organizationExperiences', 'curriculumVitae.skill.softSkill', 'curriculumVitae.finalTask.title', 'curriculumVitae.finalTask.status', 'curriculumVitae.finalTask.desc', 'curriculumVitae.achievements', 'curriculumVitae.socialMedia.linkedIn', 'curriculumVitae.socialMedia.instagram', 'curriculumVitae.socialMedia.twitter', 'curriculumVitae.socialMedia.youtube', 'curriculumVitae.portfolio.portfolioLink', 'curriculumVitae.portfolio.creativeCVFileName', 'curriculumVitae.generatedCVFileName', 'curriculumVitae.finalTask.chapterProgress', 'curriculumVitae.finalTask.totalChapter', 'curriculumVitae.photoCVFileName', 'curriculumVitae.projectExperiences', 'curriculumVitae.skill.areaOfInterest', 'curriculumVitae.skill.hardSkillLevel', 'curriculumVitae.certification', 'curriculumVitae.trainingCertification', 'educationLevel', 'institutionName', 'institutionCity', 'department', 'ipk', 'status', 'semester', 'type', 'companyName', 'startDate', 'endDate', 'isActive'\n",
    "]\n",
    "\n",
    "df_rsv.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsv.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsv.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spv.to_csv('*/transformed_src/df_spv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisior = pd.read_csv('*/transformed_src/df_spv.csv')\n",
    "df_supervisior = normalize_nested_column(df_supervisior, 'curriculumVitae.educationalBackgrounds')\n",
    "df_supervisior = normalize_nested_column(df_supervisior, 'curriculumVitae.workExperiences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df_supervisior.columns.tolist()\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'curriculumVitae.photoCVFileName', 'curriculumVitae.englishSkill.certificationType', 'curriculumVitae.englishSkill.certificationPublisher', 'curriculumVitae.englishSkill.certificateNumber', 'curriculumVitae.englishSkill.publishDate', 'curriculumVitae.englishSkill.expireDate', 'curriculumVitae.englishSkill.score', 'curriculumVitae.organizationExperiences', 'curriculumVitae.skill.hardSkillDesc', 'curriculumVitae.projectExperiences', 'curriculumVitae.skill.areaOfInterest', 'curriculumVitae.skill.hardSkillLevel', 'curriculumVitae.skill.softSkill', 'curriculumVitae.finalTask.title', 'curriculumVitae.finalTask.status', 'curriculumVitae.finalTask.chapterProgress', 'curriculumVitae.finalTask.totalChapter', 'curriculumVitae.finalTask.desc', 'curriculumVitae.achievements', 'curriculumVitae.certification', 'curriculumVitae.trainingCertification', 'curriculumVitae.socialMedia.linkedIn', 'curriculumVitae.socialMedia.instagram', 'curriculumVitae.socialMedia.twitter', 'curriculumVitae.socialMedia.youtube', 'curriculumVitae.portfolio.portfolioLink', 'curriculumVitae.portfolio.creativeCVFileName', 'curriculumVitae.generatedCVFileName', 'educationLevel', 'institutionName', 'institutionCity', 'department', 'ipk', 'status', 'semester', 'type', 'companyName', 'startDate', 'endDate', 'isActive'\n",
    "]\n",
    "\n",
    "df_supervisior.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisior.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisior.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the DataFrames (df_supervisior and df_rsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentee_participant = pd.merge(df_supervisior, df_rsv, on='participant_id', how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentee_participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the DataFrames (df_req_new and df_mentee_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_participant = pd.merge(df_req_new, df_mentee_participant, on='spv_name', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_participant.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns in df_final_participan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_participant.rename(columns={\n",
    "    \"_id\": \"request_id\",\n",
    "    \"supervisor_nik\": \"supervisor_nik\",\n",
    "    \"spv_name\": \"supervisor_name\",\n",
    "    \"jobPosition\": \"job_position\",\n",
    "    \"jobDesc\": \"job_description\",\n",
    "    \"jobSpec\": \"job_specification\",\n",
    "    \"tools\": \"required_tools\",\n",
    "    \"skills\": \"required_skills\",\n",
    "    \"internshipRole._id_x\": \"internship_role_id\",\n",
    "    \"internshipRole.title_x\": \"internship_role_title\",\n",
    "    \"spv_id\": \"supervisor_id\",\n",
    "    \"participant_id\": \"participant_id\",\n",
    "    \"participant_name_x\": \"participant_name\",\n",
    "    \"acceptedInternshipRole._id\": \"accepted_role_id\",\n",
    "    \"acceptedInternshipRole.title\": \"accepted_role_title\",\n",
    "    \"curriculumVitae.skill.hardSkillCategory_x\": \"participant_skill\",\n",
    "    \"curriculumVitae.skill.tools_x\": \"participants_tools\",\n",
    "    # \"curriculumVitae.skill.hardSkillDesc_x\": \"participant_specification\",\n",
    "    \"position_x\": \"internship_position\",\n",
    "    \"description_x\": \"internship_description\",\n",
    "    \"reserved_id\": \"reserved_id\",\n",
    "    \"reserve_name\": \"reserved_name\",\n",
    "    \"participant_name_y\": \"booked_participant_name\",\n",
    "    \"internshipRole._id_y\": \"booked_role_id\",\n",
    "    \"internshipRole.title_y\": \"booked_role_title\",\n",
    "    \"curriculumVitae.skill.tools_y\": \"booked_tools\",\n",
    "    \"curriculumVitae.skill.hardSkillCategory_y\": \"booked_skill\",\n",
    "    # \"curriculumVitae.skill.hardSkillDesc_y\": \"booked_specification\",\n",
    "    \"position_y\": \"booked_position\",\n",
    "    \"description_y\": \"booked_description\"\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final_participant.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Lokasi output\n",
    "output_loc = \"*/transformed_src/\"\n",
    "output_file = os.path.join(output_loc, \"df_final_participant.xlsx\")\n",
    "\n",
    "# Pastikan folder tujuan ada\n",
    "if not os.path.exists(output_loc):\n",
    "    os.makedirs(output_loc)\n",
    "\n",
    "# Simpan ke Excel\n",
    "df_final_participant.to_excel(output_file, index=False)\n",
    "\n",
    "\n",
    "print(f\"File berhasil disimpan di {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating Final Participant Data by Participant Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_final = df_final_participant.groupby(['participant_name'], as_index=False).agg({\n",
    "    'internship_position': lambda x: '\\n'.join(\n",
    "        f\"{i+1}. {pos}\" for i, pos in enumerate(sorted(x.fillna('Tidak ada data').unique()))\n",
    "    ),\n",
    "    'booked_position': lambda x: '\\n'.join(\n",
    "        f\"{i+1}. {pos}\" for i, pos in enumerate(sorted(x.fillna('Tidak ada data').unique()))\n",
    "    ),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the DataFrames (df_final_participant and df_merged_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant = pd.merge(df_final_participant, df_merged_final, on='participant_name', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Renaming Columns in df_fix_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_fix_participant['booked_position_x'] = df_fix_participant['booked_position_y']\n",
    "\n",
    "\n",
    "df_fix_participant['internship_position_x'] = df_fix_participant['internship_position_y']\n",
    "\n",
    "\n",
    "df_fix_participant = df_fix_participant.drop(columns=['booked_position_y', 'internship_position_y'])\n",
    "\n",
    "\n",
    "df_fix_participant = df_fix_participant.rename(columns={'booked_position_x': 'booked_position', \n",
    "                                                        'internship_position_x': 'internship_position'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df_fix_participant.columns.tolist()\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'job_description', 'internship_description', 'booked_description'\n",
    "]\n",
    "\n",
    "df_fix_participant.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df_fix_participant.columns.tolist()\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Dropping Rows in df_fix_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List kolom yang harus kosong\n",
    "kolom_kosong = [\n",
    "    'request_id', 'supervisor_nik', 'job_position', 'job_specification',\n",
    "    'required_tools', 'required_skills', 'internship_role_id', 'internship_role_title'\n",
    "]\n",
    "\n",
    "# List kolom yang harus terisi\n",
    "kolom_terisi = [\n",
    "    'supervisor_name', 'supervisor_id', 'participant_id', 'participant_name',\n",
    "    'accepted_role_id', 'accepted_role_title', 'participant_skill', 'participants_tools',\n",
    "    'internship_position', 'reserve_id', 'reserved_name', 'booked_participant_name',\n",
    "    'booked_role_id', 'booked_role_title', 'booked_skill', 'booked_tools', 'booked_position'\n",
    "]\n",
    "\n",
    "# Filter baris sesuai kondisi\n",
    "drop_cases = df_fix_participant[\n",
    "    df_fix_participant[kolom_kosong].isna().all(axis=1) &\n",
    "    df_fix_participant[kolom_terisi].notna().all(axis=1)\n",
    "]\n",
    "\n",
    "# Menampilkan jumlah baris yang di-drop\n",
    "print(f\"Jumlah baris yang di-drop: {len(drop_cases)}\")\n",
    "\n",
    "# Hapus baris-baris tersebut dari df_fix_participant\n",
    "df_cleaned1 = df_fix_participant.drop(drop_cases.index)\n",
    "\n",
    "# Jika ingin mengecek hasil, bisa ditampilkan\n",
    "print(df_cleaned1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List kolom yang harus kosong\n",
    "kolom_kosong = [\n",
    "    'request_id', 'supervisor_nik', 'job_position', 'job_specification',\n",
    "    'required_tools', 'required_skills', 'internship_role_id', 'internship_role_title',\n",
    "    'reserve_id', 'reserved_name', 'booked_participant_name', 'booked_role_id',\n",
    "    'booked_role_title', 'booked_skill', 'booked_tools'\n",
    "]\n",
    "\n",
    "# Kolom yang harus terisi\n",
    "kolom_terisi = [\n",
    "    'supervisor_id', 'supervisor_name', 'participant_id', 'participant_name',\n",
    "    'accepted_role_id', 'accepted_role_title', 'participant_skill', 'participants_tools',\n",
    "    'internship_position'\n",
    "]\n",
    "\n",
    "# Filter baris yang memenuhi kondisi: request dan booking kosong, booked_position = \"1. Tidak ada data\"\n",
    "drop_cases = df_cleaned1[\n",
    "    df_cleaned1[kolom_kosong].isna().all(axis=1) & \n",
    "    (df_cleaned1['booked_position'] == \"1. Tidak ada data\") &\n",
    "    df_cleaned1[kolom_terisi].notna().all(axis=1)\n",
    "]\n",
    "\n",
    "# Menampilkan jumlah baris yang di-drop\n",
    "print(f\"Jumlah baris yang di-drop: {len(drop_cases)}\")\n",
    "\n",
    "# Hapus baris-baris tersebut dari df_cleaned1\n",
    "df_cleaned2 = df_cleaned1.drop(drop_cases.index)\n",
    "\n",
    "# Menampilkan hasil jika ingin dicek\n",
    "print(df_cleaned2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Daftar kolom yang HARUS kosong (booking & onjob)\n",
    "kolom_kosong = [\n",
    "    'supervisor_id', 'participant_name', 'participant_id',\n",
    "    'accepted_role_id', 'accepted_role_title', 'participant_skill',\n",
    "    'participants_tools', 'internship_position',\n",
    "    'reserve_id', 'reserved_name', 'booked_participant_name',\n",
    "    'booked_role_id', 'booked_role_title', 'booked_skill',\n",
    "    'booked_tools', 'booked_position'\n",
    "]\n",
    "\n",
    "# Daftar kolom yang HARUS terisi (request lengkap)\n",
    "kolom_terisi = [\n",
    "    'supervisor_name', 'request_id', 'supervisor_nik',\n",
    "    'job_position', 'job_specification', 'required_tools',\n",
    "    'required_skills', 'internship_role_id', 'internship_role_title'\n",
    "]\n",
    "\n",
    "# Filter baris yang memenuhi kondisi tersebut\n",
    "drop_cases = df_cleaned2[\n",
    "    df_cleaned2[kolom_kosong].isna().all(axis=1) &  # semua kolom_kosong harus NaN\n",
    "    df_cleaned2[kolom_terisi].notna().all(axis=1)   # semua kolom_terisi harus terisi\n",
    "]\n",
    "\n",
    "# Tampilkan jumlah baris yang akan di-drop\n",
    "print(f\"Jumlah baris yang di-drop: {len(drop_cases)}\")\n",
    "\n",
    "# Buat DataFrame baru dengan baris-baris tersebut dihapus\n",
    "df_fix_cleaned = df_cleaned2.drop(drop_cases.index)\n",
    "\n",
    "# Opsional: tampilkan 5 baris pertama untuk cek hasil\n",
    "print(df_fix_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned = df_fix_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolom yang harus terisi agar status = 'diterima'\n",
    "request_and_on_job_cols = [\n",
    "    'request_id', 'supervisor_nik', 'supervisor_name', 'job_position', 'job_specification',\n",
    "    'required_tools', 'required_skills', 'internship_role_id', 'internship_role_title',\n",
    "    'supervisor_id', 'participant_id', 'participant_name', 'accepted_role_id',\n",
    "    'accepted_role_title', 'participant_skill', 'participants_tools', 'internship_position'\n",
    "]\n",
    "\n",
    "def set_status(row):\n",
    "    if row[request_and_on_job_cols].notna().all():\n",
    "        return 'diterima'\n",
    "    else:\n",
    "        return 'ditolak'\n",
    "\n",
    "# Tambahkan kolom status\n",
    "df_fix_participant_cleaned['status'] = df_fix_participant_cleaned.apply(set_status, axis=1)\n",
    "\n",
    "# Kosongkan participant_id jika status ditolak\n",
    "df_fix_participant_cleaned.loc[df_fix_participant_cleaned['status'] == 'ditolak', 'participant_id'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_reserved_df = df_fix_participant_cleaned[[\n",
    "    'booked_participant_name',\n",
    "    'booked_skill',\n",
    "    'booked_tools',\n",
    "    'booked_role_title',\n",
    "    'booked_position',\n",
    "    'reserved_name'\n",
    "    \n",
    "]].copy()\n",
    "\n",
    "\n",
    "booked_reserved_df = booked_reserved_df.rename(columns={\n",
    "    'booked_participant_name': 'participant_name',\n",
    "    'booked_skill': 'participant_skill',\n",
    "    'booked_tools': 'participants_tools',\n",
    "    'booked_role_title' :'accepted_role_title',\n",
    "    'booked_position': 'internship_position',\n",
    "    'reserved_name': 'supervisor_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "booked_reserved_df = df_fix_participant_cleaned[[\n",
    "    'booked_participant_name',\n",
    "    'booked_skill',\n",
    "    'booked_tools',\n",
    "    'booked_role_title',\n",
    "    'booked_position',\n",
    "    'reserved_name'\n",
    "    \n",
    "]].copy()\n",
    "\n",
    "booked_reserved_df = booked_reserved_df.rename(columns={\n",
    "    'booked_participant_name': 'participant_name',\n",
    "    'booked_skill': 'participant_skill',\n",
    "    'booked_tools': 'participants_tools',\n",
    "    'booked_role_title' :'accepted_role_title',\n",
    "    'booked_position': 'internship_position',\n",
    "    'reserved_name': 'supervisor_name'\n",
    "})\n",
    "\n",
    "df_fix_participant_cleaned = pd.concat([df_fix_participant_cleaned, booked_reserved_df], ignore_index=True)\n",
    "\n",
    "df_fix_participant_cleaned = df_fix_participant_cleaned.drop(columns=[\n",
    "    'booked_participant_name',\n",
    "    'booked_skill',\n",
    "    'booked_tools',\n",
    "    'booked_role_title',\n",
    "    'booked_position',\n",
    "    'reserved_name'\n",
    "])\n",
    "\n",
    "print(df_fix_participant_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned = df_fix_participant_cleaned[df_fix_participant_cleaned['status'] != 'ditolak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned['status'] = df_fix_participant_cleaned['status'].fillna('ditolak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kolom yang ingin di-drop\n",
    "cols_to_drop = [\n",
    "    'request_id', 'supervisor_nik', 'internship_role_id',\n",
    "    'supervisor_id', 'participant_id', 'accepted_role_id',\n",
    "    'reserve_id', 'booked_role_id'\n",
    "]\n",
    "\n",
    "# Drop kolom dari DataFrame\n",
    "df_fix_participant_cleaned = df_fix_participant_cleaned.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_fix_participant_cleaned = df_fix_participant_cleaned.rename(columns={\n",
    "    'supervisor_name': 'spv_mentee',\n",
    "    'participant_name': 'mentee_name',\n",
    "    'accepted_role_title': 'mentee_title',\n",
    "    'participant_skill': 'mentee_skill',\n",
    "    'participants_tools': 'mentee_tools',\n",
    "    'internship_position': 'mentee_position',\n",
    "    'internship_role_title': 'required_role_title',\n",
    "    'status': 'mentee_status'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sebelum = len(df_fix_participant_cleaned)\n",
    "\n",
    "# Filter data\n",
    "df_fix_participant_cleaned = df_fix_participant_cleaned[df_fix_participant_cleaned['mentee_position'] != '1. Tidak ada data']\n",
    "\n",
    "# Hitung total baris setelah filter\n",
    "total_setelah = len(df_fix_participant_cleaned)\n",
    "\n",
    "# Hitung jumlah baris yang dihapus\n",
    "baris_dihapus = total_sebelum - total_setelah\n",
    "\n",
    "print(f\"Jumlah baris yang dihapus: {baris_dihapus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari duplikat berdasarkan kolom 'booked_participant_name'\n",
    "duplicates = df_fix_participant_cleaned[df_fix_participant_cleaned.duplicated(subset=['mentee_name'], keep=False)]\n",
    "\n",
    "# Tampilkan duplikat\n",
    "print(\"Daftar data yang terduplikasi di kolom 'booked_participant_name':\")\n",
    "print(duplicates[['mentee_name']])\n",
    "\n",
    "# Hitung jumlah duplikat (unik yang muncul lebih dari 1x)\n",
    "duplicate_count = df_fix_participant_cleaned['mentee_name'].value_counts()\n",
    "duplicate_count = duplicate_count[duplicate_count > 1]\n",
    "\n",
    "print(\"\\nJumlah nama yang duplikat:\", len(duplicate_count))\n",
    "print(\"\\nDetail jumlah kemunculan tiap nama yang duplikat:\")\n",
    "print(duplicate_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hapus duplikat pada kolom 'booked_participant_name', kecuali yang kosong\n",
    "# df_fix_participant_cleaned = df_fix_participant_cleaned[df_fix_participant_cleaned['mentee_name'].notna() | \n",
    "#                                                          ~df_fix_participant_cleaned.duplicated(subset=['mentee_name'], keep='first')]\n",
    "\n",
    "df_fix_participant_cleaned = df_fix_participant_cleaned.drop_duplicates(subset=['mentee_name'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Lokasi output\n",
    "output_loc = \"*/transformed_src/\"\n",
    "output_file = os.path.join(output_loc, \"df_fix_participant_cleaned.xlsx\")\n",
    "\n",
    "# Pastikan folder tujuan ada\n",
    "if not os.path.exists(output_loc):\n",
    "    os.makedirs(output_loc)\n",
    "\n",
    "# Simpan ke Excel\n",
    "df_fix_participant_cleaned.to_excel(output_file, index=False)\n",
    "\n",
    "\n",
    "print(f\"File berhasil disimpan di {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment\n",
    "\n",
    "# Load dataset\n",
    "df_fix_participant_cleaned = pd.read_excel(\"*/transformed_src/df_fix_participant_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Daftar kolom yang ingin dibersihkan dan diatur wrapping text\n",
    "columns_to_clean = df.columns  # Membersihkan semua kolom\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Jika NaN atau None, return kosong\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)  # Pastikan dalam bentuk string\n",
    "    text = re.sub(r'[\\[\\]\\(\\){}<>]', '', text)  # Hapus tanda kurung [] () {} <>\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,:\\- ]', '', text)  # Pertahankan karakter tertentu\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Hilangkan spasi berlebih\n",
    "    \n",
    "    return text.replace(',', '\\n')  # Ubah koma menjadi baris baru\n",
    "\n",
    "# Terapkan fungsi pembersihan pada semua kolom\n",
    "df_fix_participant_cleaned = df_fix_participant_cleaned.applymap(clean_text)\n",
    "\n",
    "# Simpan hasil sementara ke Excel\n",
    "output_file = \"*/transformed_src/data_fix_participant_cleaned.xlsx\"\n",
    "df_fix_participant_cleaned.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Load file Excel untuk pengaturan wrapping text\n",
    "wb = load_workbook(output_file)\n",
    "ws = wb.active\n",
    "\n",
    "# Atur lebar kolom agar sesuai dengan teks dan aktifkan wrap text\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = get_column_letter(col[0].column)  # Dapatkan huruf kolom\n",
    "    for cell in col:\n",
    "        try:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "            cell.alignment = Alignment(wrap_text=True)  # Aktifkan wrap text\n",
    "        except:\n",
    "            pass\n",
    "    adjusted_width = min(max_length + 2, 50)  # Sesuaikan lebar kolom (max 50)\n",
    "    ws.column_dimensions[col_letter].width = adjusted_width\n",
    "\n",
    "# Simpan kembali dengan pengaturan wrapping text\n",
    "wb.save(output_file)\n",
    "\n",
    "print(f\"File berhasil dibuat dengan teks yang lebih rapi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import Library ===\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# === Load Data ===\n",
    "df = pd.read_excel(\"*/transformed_src/data_fix_participant_cleaned.xlsx\")  # ganti sesuai lokasi file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghitung Mentee dan Mentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlah_stakeholder = {\n",
    "    \"Jumlah Unik Mentor\": df['spv_mentee'].nunique(),\n",
    "    \"Jumlah Unik Mentee\": df['mentee_name'].nunique(),\n",
    "}\n",
    "\n",
    "# Jumlah unik supervisor & mentee\n",
    "jumlah_supervisor = df['spv_mentee'].nunique()\n",
    "jumlah_mentee = df['mentee_name'].nunique()\n",
    "\n",
    "print(f\"Jumlah unik supervisor: {jumlah_supervisor}\")\n",
    "print(f\"Jumlah unik mentee: {jumlah_mentee}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbandingan supervisor yang onjob vs tidak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor yang onjob-kan mentee\n",
    "supervisor_onjob = df[df['mentee_status'] == 'diterima']['spv_mentee'].dropna().unique()\n",
    "\n",
    "# Supervisor yang tidak onjob-kan mentee\n",
    "supervisor_not_onjob = df[df['mentee_status'] == 'ditolak']['spv_mentee'].dropna().unique()\n",
    "\n",
    "# Untuk menghindari tumpang tindih, kita ambil supervisor yang hanya ada di 'ditolak' dan tidak di 'diterima'\n",
    "supervisor_murni_tidak_onjob = set(supervisor_not_onjob) - set(supervisor_onjob)\n",
    "\n",
    "# Hitung jumlahnya\n",
    "print(f\"Jumlah supervisor yang onjob-kan mentee: {len(supervisor_onjob)}\")\n",
    "print(f\"Jumlah supervisor yang tidak onjob-kan mentee: {len(supervisor_murni_tidak_onjob)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Supervisor Onjob', 'Supervisor Tidak Onjob']\n",
    "values = [len(supervisor_onjob), len(supervisor_murni_tidak_onjob)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, values, color=['green', 'red'])\n",
    "plt.title('Perbandingan Supervisor yang Onjob vs Tidak Onjob-kan Mentee')\n",
    "plt.ylabel('Jumlah Supervisor')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daftar nama supervisor yang onjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil supervisor yang punya mentee dengan status diterima\n",
    "supervisor_onjob = df[df['mentee_status'] == 'diterima']['spv_mentee'].dropna().unique()\n",
    "\n",
    "# Ubah jadi list dan urutkan\n",
    "supervisor_onjob_sorted = sorted(supervisor_onjob)\n",
    "\n",
    "# Tampilkan\n",
    "print(\"Supervisor yang onjob-kan mentee:\")\n",
    "for spv in supervisor_onjob_sorted:\n",
    "    print(\"-\", spv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribusi role yang tersedia vs onjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jumlah_role = {\n",
    "    \"Jumlah Role Magang Tersedia\": df['required_role_title'].nunique(),\n",
    "    \"Jumlah Role yang Onjob\" : df['mentee_title'].nunique()\n",
    "}\n",
    "\n",
    "# Tampilkan distribusi role yang dibooking\n",
    "print(df['required_role_title'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(df['mentee_title'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Histogram Role yang onjob\n",
    "top_roles = df['mentee_title'].value_counts().head(10)\n",
    "sns.barplot(x=top_roles.values, y=top_roles.index, palette='Blues_d')\n",
    "plt.title('Top 10 Role yang Paling Banyak Onjob')\n",
    "plt.xlabel('Jumlah Mentee Booked')\n",
    "plt.ylabel('Role Onjob')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jumlah role mentee yang diterima oleh spv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hanya mentee yang diterima\n",
    "df_diterima = df[df['mentee_status'] == 'diterima']\n",
    "\n",
    "# Hitung jumlah masing-masing mentee_title yang diterima\n",
    "role_diterima_counts = df_diterima['mentee_title'].value_counts()\n",
    "\n",
    "# Tampilkan hasilnya\n",
    "print(\"Jumlah masing-masing role mentee_title yang diterima:\")\n",
    "print(role_diterima_counts)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "role_diterima_counts.plot(kind='barh', color='skyblue')\n",
    "plt.title('Jumlah Role Mentee (mentee_title) yang Diterima')\n",
    "plt.xlabel('Jumlah Diterima')\n",
    "plt.ylabel('Role Mentee')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()  # agar yang paling banyak ada di atas\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 role diterima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hanya yang statusnya 'diterima'\n",
    "df_diterima = df_fix_participant_cleaned[df_fix_participant_cleaned['mentee_status'] == 'diterima']\n",
    "\n",
    "# Hitung jumlah masing-masing mentee_title\n",
    "top_5_roles = df_diterima['mentee_title'].value_counts().head(5)\n",
    "\n",
    "# Tampilkan hasilnya\n",
    "print(\"Top 5 Role (mentee_title) yang Paling Banyak Diterima:\")\n",
    "print(top_5_roles)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(y=top_5_roles.index, x=top_5_roles.values, palette='Blues_d')\n",
    "plt.title('Top 5 Role Mentee yang Paling Banyak Diterima')\n",
    "plt.xlabel('Jumlah Diterima')\n",
    "plt.ylabel('Role (mentee_title)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghitung perbandingan Mentee booked dan Onjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hanya mentee yang diterima\n",
    "df_diterima = df[df['mentee_status'] == 'diterima']\n",
    "\n",
    "# Hitung jumlah masing-masing mentee_position\n",
    "mentee_position_counts = df_diterima['mentee_position'].value_counts()\n",
    "\n",
    "# Tampilkan\n",
    "print(\"Posisi Mentee (mentee_position) yang Paling Dibutuhkan Supervisor:\")\n",
    "print(mentee_position_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbandingan mentee yang booked vs onjob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hitung jumlah unik mentee\n",
    "jumlah_dibooking = df_fix_cleaned['booked_participant_name'].nunique()\n",
    "jumlah_onjob = df_fix_cleaned['participant_name'].nunique()\n",
    "\n",
    "# Tampilkan\n",
    "print(\"Jumlah mentee yang dibooking:\", jumlah_dibooking)\n",
    "print(\"Jumlah mentee yang onjob:\", jumlah_onjob)\n",
    "\n",
    "# Visualisasi perbandingan\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=['Booked', 'Onjob'], y=[jumlah_dibooking, jumlah_onjob], palette='Set2')\n",
    "plt.title('Perbandingan Jumlah Mentee Booked vs Onjob')\n",
    "plt.ylabel('Jumlah Unik Mentee')\n",
    "plt.xlabel('Status')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \tMismatch role booked vs onjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kesesuaian Role dan Realisasi Booking\n",
    "role_mismatch = df_fix_cleaned[df_fix_cleaned['internship_role_title'] != df_fix_cleaned['booked_role_title']]\n",
    "print(\"Jumlah Mismatch Role:\", len(role_mismatch))\n",
    "print(role_mismatch['internship_role_title'].value_counts().head(10))\n",
    "\n",
    "# Histogram Role Mismatch\n",
    "mismatch_counts = role_mismatch['internship_role_title'].value_counts().head(10)\n",
    "sns.barplot(x=mismatch_counts.values, y=mismatch_counts.index, palette='magma')\n",
    "plt.title('Top 10 Role dengan Mismatch Terbanyak')\n",
    "plt.xlabel('Jumlah Mismatch')\n",
    "plt.ylabel('Internship Role')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 skills & tools dimiliki mentee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items(column_data):\n",
    "    items = []\n",
    "    for entry in column_data.dropna():\n",
    "        split_items = [item.strip().lower() for item in entry.replace('\\n', ',').split(',')]\n",
    "        items.extend(split_items)\n",
    "    return Counter(items)\n",
    "\n",
    "# Skill & tools\n",
    "top_participant_skill = extract_items(df['mentee_skill']).most_common(5)\n",
    "top_participant_tools = extract_items(df['mentee_tools']).most_common(5)\n",
    "\n",
    "print(\"Top 5 Skill yang dimiliki mentee:\", top_participant_skill)\n",
    "print(\"Top 5 Tools yang dimiliki mentee:\", top_participant_tools)\n",
    "\n",
    "\n",
    "# Visualisasi\n",
    "def plot_bar_chart(data, title, color):\n",
    "    labels, values = zip(*data)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(labels, values, color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Item\")\n",
    "    plt.ylabel(\"Jumlah\")\n",
    "    \n",
    "    # Tambahkan angka di atas setiap batang\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f'{int(height)}', \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.ylim(0, max(values) + 5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot top 5 skill\n",
    "plot_bar_chart(top_participant_skill, \"Top 5 Skill yang Dimiliki Mentee\", color='skyblue')\n",
    "\n",
    "# Plot top 5 tools\n",
    "plot_bar_chart(top_participant_tools, \"Top 5 Tools yang Dimiliki Mentee\", color='lightgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 skills & tools requested by supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Fungsi yang sudah kamu punya:\n",
    "def extract_items(column_data):\n",
    "    items = []\n",
    "    for entry in column_data.dropna():\n",
    "        split_items = [item.strip().lower() for item in entry.replace('\\n', ',').split(',')]\n",
    "        items.extend(split_items)\n",
    "    return Counter(items)\n",
    "\n",
    "\n",
    "# Skill & tools yang dibutuhkan supervisor\n",
    "top_required_skill = extract_items(df['required_skills']).most_common(5)\n",
    "top_required_tools = extract_items(df['required_tools']).most_common(5)\n",
    "\n",
    "print(\"Top 5 Skill yang dibutuhkan supervisor:\", top_required_skill)\n",
    "print(\"Top 5 Tools yang dibutuhkan supervisor:\", top_required_tools)\n",
    "\n",
    "\n",
    "#Pisahkan data jadi 2 list: skill/tools dan frekuensinya\n",
    "skills, skill_counts = zip(*top_required_skill)\n",
    "tools, tool_counts = zip(*top_required_tools)\n",
    "\n",
    "# Buat histogram (bar chart) untuk Skill\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)  # Subplot pertama untuk Skill\n",
    "plt.bar(skills, skill_counts, color='skyblue')\n",
    "plt.xlabel('Skill')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.title('Top 5 Skill yang Dibutuhkan Supervisor')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Buat histogram (bar chart) untuk Tools\n",
    "plt.subplot(1, 2, 2)  # Subplot kedua untuk Tools\n",
    "plt.bar(tools, tool_counts, color='salmon')\n",
    "plt.xlabel('Tools')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.title('Top 5 Tools yang Dibutuhkan Supervisor')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribusi mentee_status (diterima vs ditolak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hitung jumlah berdasarkan status\n",
    "jumlah_status = df_fix_participant_cleaned['mentee_status'].value_counts()\n",
    "\n",
    "# Plot histogram (bar chart)\n",
    "plt.figure(figsize=(6, 4))\n",
    "jumlah_status.plot(kind='bar', color=['green', 'red'])\n",
    "\n",
    "# Tambahan info visual\n",
    "plt.title('Jumlah Peserta Diterima vs Ditolak')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Jumlah Peserta')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Tampilkan nilai di atas bar\n",
    "for i, val in enumerate(jumlah_status):\n",
    "    plt.text(i, val + 1, str(val), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booking vs Onjob overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ambil nama mentee yang unik dan tidak kosong\n",
    "booked = set(df_fix_cleaned['booked_participant_name'].dropna().unique())\n",
    "onjob = set(df_fix_cleaned['participant_name'].dropna().unique())\n",
    "\n",
    "# 1. Mentee yang booking dan juga on-job\n",
    "booked_and_onjob = booked & onjob\n",
    "\n",
    "# 2. Mentee yang booking tapi tidak on-job\n",
    "booked_only = booked - onjob\n",
    "\n",
    "# 3. Mentee yang on-job tapi tidak booking\n",
    "onjob_only = onjob - booked\n",
    "\n",
    "# Tampilkan jumlah\n",
    "print(f\"1. Booking & On-job        : {len(booked_and_onjob)} orang\")\n",
    "print(f\"2. Booking tapi bukan On-job: {len(booked_only)} orang\")\n",
    "print(f\"3. On-job tapi bukan Booking: {len(onjob_only)} orang\")\n",
    "\n",
    "# Histogram\n",
    "labels = ['Booking & On-job', 'Booking Only', 'On-job Only']\n",
    "counts = [len(booked_and_onjob), len(booked_only), len(onjob_only)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(labels, counts, color=['green', 'orange', 'blue'])\n",
    "\n",
    "# Tambahkan label jumlah di atas tiap bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.5, int(yval), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Distribusi Mentee Booking vs On-job')\n",
    "plt.ylabel('Jumlah Mentee')\n",
    "plt.xlabel('Kategori')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gabungkan semua skill menjadi satu string\n",
    "required_skills = ','.join(df['required_skills'].dropna().tolist())\n",
    "mentee_skill = ','.join(df['mentee_skill'].dropna().tolist())\n",
    "mentee_tools = ','.join(df['mentee_tools'].dropna().tolist())\n",
    "required_tools = ','.join(df['required_tools'].dropna().tolist())\n",
    "\n",
    "# Buat WordCloud\n",
    "wordcloud_A = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(required_skills)\n",
    "wordcloud_B = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(mentee_skill)\n",
    "wordcloud_C = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(mentee_tools)\n",
    "wordcloud_D = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(required_tools)\n",
    "\n",
    "# Tampilkan\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_A, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"WordCloud: Required Skills dari Mentor\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_B, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"WordCloud: Skill dari Mentee\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_C, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"WordCloud: Tools dari Mentee\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_D, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"WordCloud: Required Tools dari Mentor\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Hapus definisi variabel 'list' kalau pernah tertimpa\n",
    "try:\n",
    "    del list\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Fungsi untuk memecah teks panjang jadi list poin yang lebih rapi\n",
    "def split_into_list(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = re.sub(r'(?<=\\d)\\.(?=\\S)', '. ', text)\n",
    "    text = re.sub(r'(?<=-)(?=\\S)', ' ', text)\n",
    "    parts = re.split(r'(?:\\d+\\.\\s*|-|\\n)', text)\n",
    "    cleaned = [part.strip() for part in parts if part.strip()]\n",
    "    return cleaned\n",
    "\n",
    "# Fungsi untuk ubah list jadi teks multiline\n",
    "def list_to_multiline_text(lst):\n",
    "    if not isinstance(lst, list):\n",
    "        return \"\"\n",
    "    return '\\n'.join(lst)\n",
    "\n",
    "# Terapkan ke dataframe kamu\n",
    "df_fix_participant_cleaned['job_specification'] = df_fix_participant_cleaned['job_specification'].apply(\n",
    "    lambda x: list_to_multiline_text(split_into_list(x))\n",
    ")\n",
    "\n",
    "df_fix_participant_cleaned['mentee_position'] = df_fix_participant_cleaned['mentee_position'].apply(\n",
    "    lambda x: list_to_multiline_text(split_into_list(x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_participant_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowering the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowercased = df_fix_participant_cleaned.applymap(\n",
    "    lambda x: x.lower() if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'job_specification'\n",
    "]\n",
    "\n",
    "df_lowercased.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing remove non-word and non-whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowercased = df_lowercased.replace(to_replace=r'[^\\w\\s]', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Fungsi ubah list jadi multiline text dengan bullet\n",
    "def list_to_multiline_text(lst):\n",
    "    if isinstance(lst, list):\n",
    "        return '\\n'.join(f\" {item}\" for item in lst)\n",
    "    return lst\n",
    "\n",
    "# Terapkan fungsi ke semua kolom di df_lowercased yang berisi list\n",
    "for col in df_lowercased.columns:\n",
    "    if df_lowercased[col].apply(lambda x: isinstance(x, list)).any():\n",
    "        df_lowercased[col] = df_lowercased[col].apply(list_to_multiline_text)\n",
    "\n",
    "# Lokasi penyimpanan\n",
    "output_loc = \"*/transformed_src/\"\n",
    "output_file = os.path.join(output_loc, \"data_participant_preprocess_formatted.xlsx\")\n",
    "\n",
    "# Pastikan folder ada\n",
    "os.makedirs(output_loc, exist_ok=True)\n",
    "\n",
    "# Simpan ke Excel pakai xlsxwriter dan wrap text\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    df_lowercased.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Format wrap text agar isi list tampil ke bawah dalam satu sel\n",
    "    wrap_format = workbook.add_format({'text_wrap': True, 'valign': 'top'})\n",
    "\n",
    "    # Terapkan format dan lebarkan semua kolom\n",
    "    for idx, col in enumerate(df_lowercased.columns):\n",
    "        worksheet.set_column(idx, idx, 30, wrap_format)\n",
    "\n",
    "print(f\" File berhasil disimpan dengan rapi di: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_loc = \"*/transformed_src/\"\n",
    "output_path = os.path.join(output_loc, 'df_lowercased.csv')\n",
    "df_lowercased.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV asli\n",
    "df_lowercased = pd.read_csv('*/app/data/mentee_history.csv')\n",
    "\n",
    "# Copy dulu supaya df_lowercased tetap aman\n",
    "df_new_mentee = df_lowercased.copy()\n",
    "\n",
    "# Hapus kolom mentee_status di df_new_mentee\n",
    "df_new_mentee.drop(columns=['mentee_status'], inplace=True)\n",
    "\n",
    "# Simpan df_new_mentee ke CSV baru\n",
    "df_new_mentee.to_csv('*/app/data/new_mentee.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_mentee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation for TF-IDF Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt', download_dir='*/nltk_data_tfidf')\n",
    "nltk.download('wordnet', download_dir='*/nltk_data_tfidf')\n",
    "nltk.download('stopwords', download_dir='*/nltk_data_tfidf')\n",
    "nltk.download('punkt_tab', download_dir='*/nltk_data_tfidf')\n",
    "\n",
    "# Setelah itu, beri tahu NLTK lokasi tersebut\n",
    "nltk.data.path.append('*/nltk_data_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wbl_dataset(filepath, verbose=False):\n",
    "\n",
    "    # Inisialisasi preprocessing tools\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text.lower())\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    text_cols = [\n",
    "        'spv_mentee', 'job_position', 'required_tools', 'required_skills', \n",
    "        'required_role_title', 'mentee_name', 'mentee_title', 'mentee_skill',\n",
    "        'mentee_tools', 'mentee_position'\n",
    "    ]\n",
    "    df.fillna('', inplace=True)\n",
    "    df['combined_text'] = df[text_cols].agg(' '.join, axis=1)\n",
    "\n",
    "    # Terapkan preprocessing di sini, ke semua teks gabungan\n",
    "    df['combined_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "    y = df['mentee_status']\n",
    "    target_names = y.unique()\n",
    "\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=2)\n",
    "    X = vectorizer.fit_transform(df['combined_text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    duration = time() - t0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Dokumen: {len(df)}, Fitur: {X.shape[1]}\")\n",
    "        print(f\"TF-IDF selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "    return X, y, feature_names, target_names, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_wbl_dataset(filepath, verbose=False):\n",
    "#     \"\"\"\n",
    "#     Load and vectorize the WBL mentee-mentor matching dataset.\n",
    "    \n",
    "#     Param:\n",
    "#         filepath (str): Path ke CSV file data kamu.\n",
    "#         verbose (bool): Untuk menampilkan informasi proses.\n",
    "\n",
    "#     Return:\n",
    "#         X (TF-IDF matrix), y (labels), feature_names (list of terms), target_names (target labels), df (dataframe asli)\n",
    "#     \"\"\"\n",
    "#     # 1. Load data kamu dari CSV\n",
    "#     df = pd.read_csv(filepath)\n",
    "\n",
    "#     # 2. Gabungkan semua kolom teks relevan jadi satu kolom 'combined_text'\n",
    "#     text_cols = [\n",
    "#         'spv_mentee', 'job_position', 'required_tools', 'required_skills', \n",
    "#         'required_role_title', 'mentee_name', 'mentee_title', 'mentee_skill',\n",
    "#         'mentee_tools', 'mentee_position'\n",
    "#     ]\n",
    "#     df.fillna('', inplace=True)  # Hindari NaN\n",
    "#     df['combined_text'] = df[text_cols].agg(' '.join, axis=1)\n",
    "\n",
    "#     # 3. Target labels (misalnya 'mentee_status' untuk label target)\n",
    "#     y = df['mentee_status']\n",
    "#     target_names = y.unique()  # Mendapatkan nama kategori target (misalnya status mentee)\n",
    "    \n",
    "#     # 4. TF-IDF vectorization\n",
    "#     t0 = time()\n",
    "#     vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=2, stop_words='english')\n",
    "#     X = vectorizer.fit_transform(df['combined_text'])\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     duration = time() - t0\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Dokumen: {len(df)}, Fitur: {X.shape[1]}\")\n",
    "#         print(f\"TF-IDF selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "#     return X, y, feature_names, target_names, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names, target_names, df_raw = load_wbl_dataset(\"*/transformed_src/df_lowercased.csv\", verbose=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation for Word2Vec Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load file yang sudah disimpan\n",
    "file_path = \"*/transformed_src/df_lowercased.csv\"\n",
    "df_lowercased = pd.read_csv(file_path)\n",
    "\n",
    "# Gabungkan kolom-kolom menjadi satu kolom 'text'\n",
    "df_lowercased['text'] = df_lowercased[['spv_mentee', 'job_position', 'required_tools', \n",
    "                                       'required_skills', 'required_role_title', 'mentee_name', \n",
    "                                       'mentee_title', 'mentee_skill', 'mentee_tools', \n",
    "                                       'mentee_position', 'mentee_status']\n",
    "                                      ].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Buat dataframe akhir untuk modeling\n",
    "df_combined = df_lowercased[['text', 'mentee_status']].rename(columns={'mentee_status': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_combined['labels'] = encoder.fit_transform(df_combined['labels'])\n",
    "\n",
    "X = df_combined['text']\n",
    "y = df_combined['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Hapus semua path sebelumnya\n",
    "nltk.data.path.clear()\n",
    "\n",
    "# Tambahkan path yang benar\n",
    "nltk.data.path.append('*/nltk_data')\n",
    "\n",
    "# Verifikasi path yang digunakan\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.data.path.append('*/nltk_data')\n",
    "\n",
    "nltk.download('punkt', download_dir='*/nltk_data')\n",
    "nltk.download('stopwords', download_dir='*/nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='*/nltk_data')\n",
    "\n",
    "# Fungsi preprocessing tanpa stemming\n",
    "def preprocess(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Menghapus karakter non-alfabet\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Menurunkan semua huruf ke kecil\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tentukan bahasa berdasarkan kehadiran kata-kata bahasa Indonesia atau Inggris\n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    stop_words_id = set(stopwords.words('indonesian'))\n",
    "    \n",
    "    # Tentukan stopwords dan bahasa\n",
    "    words = nltk.word_tokenize(text)\n",
    "    language = 'id' if any(word in stop_words_id for word in words) else 'en'\n",
    "\n",
    "    if language == 'en':\n",
    "        stop_words = stop_words_en\n",
    "    else:\n",
    "        stop_words = stop_words_id\n",
    "    \n",
    "    # Tokenisasi dan hapus stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train)\n",
    "X_test = pd.Series(X_test)\n",
    "\n",
    "# Tidak usah isi NaN, biarkan tetap kosong\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "WINDOW = 50\n",
    "MIN_COUNT = 5\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, vector_size=VECTOR_SIZE, window=WINDOW, min_count=MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    return np.mean(words_vecs, axis=0)\n",
    "\n",
    "X_train_vec = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test_vec = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling dan Evaluation : Text Classification dengan TF-IDF dan Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Model Klasifikasi dengan Representasi TF-IDF dan 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "# Data\n",
    "X = df_raw['combined_text']\n",
    "y = df_raw['mentee_status']\n",
    "\n",
    "# TF-IDF vektorisasi\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=2, stop_words=\"english\")\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Daftar model yang akan diuji\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Ridge Classifier\": RidgeClassifier(tol=1e-2, solver=\"sparse_cg\"),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Linear SVC\": LinearSVC(max_iter=1000),\n",
    "    \"Log-loss SGD\": SGDClassifier(loss=\"log_loss\", max_iter=1000, tol=1e-3, random_state=42),\n",
    "    \"Nearest Centroid\": NearestCentroid(),\n",
    "    \"Complement Naive Bayes\": ComplementNB()\n",
    "}\n",
    "\n",
    "# Dictionary untuk menyimpan hasil seluruh model\n",
    "all_results = {}\n",
    "\n",
    "# 10-fold stratified CV\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop ke semua model\n",
    "for model_name, clf in models.items():\n",
    "    print(f\"\\nEvaluating: {model_name}\")\n",
    "    results = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    }\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X_tfidf, y), 1):\n",
    "        X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        results[\"precision\"].append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "        results[\"recall\"].append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "        results[\"f1_score\"].append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "\n",
    "        print(f\"  Fold {fold}: f1={results['f1_score'][-1]:.3f}\")\n",
    "\n",
    "    # Simpan hasil rata-rata\n",
    "    all_results[model_name] = {\n",
    "        \"Mean Precision\": np.mean(results[\"precision\"]),\n",
    "        \"Mean Recall\": np.mean(results[\"recall\"]),\n",
    "        \"Mean F1-Score\": np.mean(results[\"f1_score\"])\n",
    "    }\n",
    "\n",
    "# Tampilkan hasil akhir\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Benchmarking Hasil Rata-Rata (10-Fold Cross-Validation):\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results).T  # Transpose agar lebih rapi\n",
    "print(results_df.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Perbandingan Kinerja Model (10-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Urutkan data berdasarkan F1-Score tertinggi\n",
    "results_df_sorted = results_df.sort_values(by=\"Mean F1-Score\", ascending=False)\n",
    "\n",
    "# Ambil list model dan metrik\n",
    "models = results_df_sorted.index.tolist()\n",
    "metrics = [\"Mean Precision\", \"Mean Recall\", \"Mean F1-Score\"]\n",
    "\n",
    "# Data untuk setiap metrik\n",
    "data = [results_df_sorted[metric].values for metric in metrics]\n",
    "\n",
    "# Setup posisi bar\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot bar untuk tiap metrik per model\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(index + i * bar_width, data[i], bar_width, label=metric)\n",
    "\n",
    "# Label dan judul\n",
    "ax.set_xlabel(\"Models\")\n",
    "ax.set_ylabel(\"Skor\")\n",
    "ax.set_title(\"Perbandingan Kinerja Model (10-Fold CV)\")\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "ax.set_ylim(0, 1.05)  # agar ada ruang untuk label nilai\n",
    "\n",
    "# Grid untuk memudahkan pembacaan\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6, axis='y')\n",
    "\n",
    "# Tambahkan nilai di atas setiap bar\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(metrics)):\n",
    "        ax.text(index[i] + j*bar_width, data[j][i] + 0.02, f\"{data[j][i]:.3f}\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Model Klasifikasi dengan Representasi Word2Vec dan 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# Daftar model\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(C=5, max_iter=1000),\n",
    "    \"Ridge Classifier\": RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Linear SVC\": LinearSVC(C=0.1, dual=False, max_iter=1000),\n",
    "    \"Log-loss SGD\": SGDClassifier(loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True),\n",
    "    \"Nearest Centroid\": NearestCentroid()\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_results = {}\n",
    "\n",
    "X_clean = X.apply(preprocess)\n",
    "X_w2v = np.array([vectorize(sentence) for sentence in X_clean])\n",
    "\n",
    "\n",
    "# Uji semua model\n",
    "for model_name, clf in models.items():\n",
    "    print(f\"\\nEvaluating: {model_name}\")\n",
    "    results = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": [],\n",
    "        \"train_time\": [],\n",
    "        \"test_time\": []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_w2v, y), 1):\n",
    "        X_train, X_test = X_w2v[train_idx], X_w2v[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        t0 = time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_duration = time() - t0\n",
    "\n",
    "        t0 = time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        test_duration = time() - t0\n",
    "\n",
    "        prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        results[\"precision\"].append(prec)\n",
    "        results[\"recall\"].append(rec)\n",
    "        results[\"f1_score\"].append(f1)\n",
    "        results[\"train_time\"].append(train_duration)\n",
    "        results[\"test_time\"].append(test_duration)\n",
    "\n",
    "        print(f\"  f1={f1:.3f}\")\n",
    "\n",
    "    # Simpan rata-rata hasil\n",
    "    all_results[model_name] = {\n",
    "        \"Mean Precision\": np.mean(results[\"precision\"]),\n",
    "        \"Mean Recall\": np.mean(results[\"recall\"]),\n",
    "        \"Mean F1-Score\": np.mean(results[\"f1_score\"]),\n",
    "        \"Avg Train Time\": np.mean(results[\"train_time\"]),\n",
    "        \"Avg Test Time\": np.mean(results[\"test_time\"]),\n",
    "    }\n",
    "\n",
    "# Tampilkan hasil akhir dalam bentuk DataFrame\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Benchmarking Hasil Rata-Rata (Word2Vec + 10-Fold Cross-Validation):\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "print(results_df.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Perbandingan Kinerja Model (10-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ambil data hasil benchmark\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "\n",
    "# Urutan model dan metrik\n",
    "models = results_df.index.tolist()\n",
    "metrics = ['Mean Precision', 'Mean Recall', 'Mean F1-Score']\n",
    "\n",
    "# Data untuk setiap metrik\n",
    "data = [results_df[metric].values for metric in metrics]\n",
    "\n",
    "# Setup posisi bar\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(models))\n",
    "\n",
    "# Membuat figure dan axis\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot tiap metrik sebagai bar yang berdampingan\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(index + i * bar_width, data[i], bar_width, label=metric)\n",
    "\n",
    "# Label dan judul\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Perbandingan Kinerja Model (10-Fold CV)')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.set_ylim(0, 1.05)  # supaya ada ruang di atas\n",
    "\n",
    "# Tambahkan legend\n",
    "ax.legend()\n",
    "\n",
    "# Tampilkan nilai di atas setiap bar\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(metrics)):\n",
    "        ax.text(index[i] + j*bar_width, data[j][i] + 0.02, f\"{data[j][i]:.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "colors = ['skyblue'] * len(results_df)\n",
    "\n",
    "# Visualisasi waktu training dan testing (opsional)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time\n",
    "axes[0].barh(results_df.index, results_df['Avg Train Time'], color=colors)\n",
    "axes[0].set_title('Average Training Time (seconds)')\n",
    "axes[0].invert_yaxis()\n",
    "for idx, val in enumerate(results_df['Avg Train Time']):\n",
    "    axes[0].text(val + max(results_df['Avg Train Time'])*0.01, idx, f\"{val:.3f}\", va='center')\n",
    "\n",
    "# Testing time\n",
    "axes[1].barh(results_df.index, results_df['Avg Test Time'], color=colors)\n",
    "axes[1].set_title('Average Testing Time (seconds)')\n",
    "axes[1].invert_yaxis()\n",
    "for idx, val in enumerate(results_df['Avg Test Time']):\n",
    "    axes[1].text(val + max(results_df['Avg Test Time'])*0.01, idx, f\"{val:.3f}\", va='center')\n",
    "\n",
    "plt.suptitle('Comparison of Model Computational Time', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark tiap model dengan TF-IDF:\n",
    "\n",
    "| Model                 | Mean Precision | Mean Recall | Mean F1-Score |\n",
    "|-----------------------|----------------|-------------|---------------|\n",
    "| Logistic Regression    | 0.973          | 0.970       | 0.970         |\n",
    "| **Ridge Classifier**       | 0.981          | 0.977       | **0.978**         |\n",
    "| k-Nearest Neighbors    | 0.929          | 0.909       | 0.911         |\n",
    "| **Random Forest**          | 0.982          | 0.982       | **0.981**         |\n",
    "| Linear SVC             | 0.973          | 0.970       | 0.970         |\n",
    "| Log-loss SGD           | 0.976          | 0.972       | 0.973         |\n",
    "| Nearest Centroid       | 0.932          | 0.932       | 0.930         |\n",
    "| Complement Naive Bayes | 0.960          | 0.956       | 0.957         |\n",
    "\n",
    "\n",
    "\n",
    "#### Benchmark tiap model dengan Word2Vec:\n",
    "\n",
    "| Model                 | Precision | Recall | F1-Score | Avg Train Time (s) | Avg Test Time (s) |\n",
    "|-----------------------|----------------|-------------|---------------|--------------------|-------------------|\n",
    "| Logistic Regression    | 0.672          | 0.651       | 0.646         | 0.005              | 0.000             |\n",
    "| Ridge Classifier       | 0.667          | 0.641       | 0.633         | 0.002              | 0.000             |\n",
    "| k-Nearest Neighbors    | 0.729          | 0.725       | 0.724         | 0.001              | 0.001             |\n",
    "| **Random Forest**          | 0.835          | 0.822       | **0.822**         | 0.278              | 0.006             |\n",
    "| Linear SVC             | 0.724          | 0.616       | 0.578         | 0.002              | 0.000             |\n",
    "| Log-loss SGD           | 0.350          | 0.503       | 0.343         | 0.005              | 0.000             |\n",
    "| Nearest Centroid       | 0.625          | 0.621       | 0.619         | 0.001              | 0.001             |\n",
    "\n",
    "\n",
    "###  Analisis Evaluasi Model\n",
    "\n",
    "Berdasarkan hasil benchmark dengan dua jenis representasi fitur  **TF-IDF** dan **Word2Vec**  diperoleh beberapa poin penting sebagai berikut:\n",
    "\n",
    "**1. Random Forest menunjukkan performa terbaik secara konsisten**\n",
    "- Pada representasi **TF-IDF**, Random Forest mencatat skor F1 tertinggi (**0.981**), sedikit lebih tinggi dari Ridge Classifier (**0.978**).\n",
    "- Pada **Word2Vec**, Random Forest tetap menjadi model terbaik dengan F1-Score **0.822**, unggul jauh dibanding model lainnya.\n",
    "\n",
    "**2. Ridge Classifier memberikan hasil yang kompetitif dengan TF-IDF**\n",
    "- Ridge Classifier menjadi alternatif kuat untuk TF-IDF, dengan performa tinggi serta waktu pelatihan dan pengujian yang sangat efisien.\n",
    "\n",
    "**3. Kinerja model cenderung menurun pada Word2Vec**\n",
    "- Sebagian besar model mengalami penurunan skor F1 ketika menggunakan Word2Vec, termasuk Logistic Regression dan Linear SVC.\n",
    "- Hal ini kemungkinan disebabkan oleh jumlah data yang terbatas (sekitar 300 data) serta karakteristik teks yang pendek.\n",
    "- TF-IDF yang hanya melihat seberapa sering kata muncul ternyata lebih cocok untuk kondisi ini, karena model seperti Logistic Regression dan Linear SVC jadi lebih mudah menangkap pola dari data yang sudah rapi dan terstruktur dengan baik.\n",
    "\n",
    "**4. Keterbatasan Word2Vec dalam konteks data kecil**\n",
    "- Word2Vec bekerja dengan baik jika dilatih pada dataset besar karena perlu memahami relasi antar kata secara kontekstual.\n",
    "- Dalam kasus ini, meskipun telah menggunakan versi pre-trained, model tersebut belum tentu sesuai dengan konteks spesifik dataset, sehingga representasi katanya tidak optimal dan menyebabkan performa model menjadi lebih rendah dibanding TF-IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Buat dan Simpan Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Buat ulang model Random Forest di seluruh data\n",
    "final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "final_model.fit(X_tfidf, y)\n",
    "\n",
    "# Simpan model dan vectorizer\n",
    "os.makedirs('*/model', exist_ok=True)\n",
    "\n",
    "joblib.dump(final_model, '*/model/matching-model.pkl')\n",
    "joblib.dump(vectorizer, '*/model/tf-idfvectorizer.pkl')\n",
    "\n",
    "print(\" Model dan TF-IDF vectorizer berhasil disimpan.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inspect Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the vectorizer\n",
    "vectorizer = joblib.load('*/model/tf-idfvectorizer.pkl')\n",
    "\n",
    "# Tampilkan tipe dan info umum\n",
    "print(\"Tipe model vectorizer:\", type(vectorizer))\n",
    "print(\"Jumlah kosakata (vocabulary):\", len(vectorizer.vocabulary_))\n",
    "\n",
    "# Contoh item dari vocabulary\n",
    "print(\"Contoh vocabulary:\", list(vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# Nilai IDF\n",
    "print(\"Nilai IDF:\", vectorizer.idf_[:10])\n",
    "\n",
    "# Coba vektorisasi kalimat contoh\n",
    "sample_text = [\"pengembangan sistem informasi\"]\n",
    "sample_text = list(map(lambda x: x.lower(), sample_text))  # lowercase\n",
    "features = vectorizer.transform(sample_text)\n",
    "\n",
    "# TF-IDF hasil (versi dense matrix)\n",
    "print(\"TF-IDF (dense):\", features.todense())\n",
    "\n",
    "# Fitur nama\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Nama fitur:\", feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = joblib.load('*/model/matching-model.pkl')\n",
    "\n",
    "# Tipe model\n",
    "print(\"Tipe model:\", type(model))\n",
    "\n",
    "# Parameter model\n",
    "print(\"Parameter:\", model.get_params())\n",
    "\n",
    "# Informasi tambahan jika tersedia\n",
    "if hasattr(model, 'n_features_in_'):\n",
    "    print(\"Jumlah fitur input:\", model.n_features_in_)\n",
    "\n",
    "if hasattr(model, 'classes_'):\n",
    "    print(\"Label Kelas:\", model.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input (disamakan formatnya)\n",
    "sample_text = [\"pengembangan sistem informasi\"]\n",
    "sample_text = list(map(lambda x: x.lower(), sample_text))\n",
    "features = vectorizer.transform(sample_text)\n",
    "\n",
    "# Prediksi probabilitas\n",
    "pred_prob = model.predict_proba(features)\n",
    "\n",
    "# Tampilkan sebagai DataFrame dengan label kelas\n",
    "import pandas as pd\n",
    "label_map = ['Data Scientist', 'Designer', 'Developer', 'Engineer', 'General', 'Marketing', 'Researcher']\n",
    "df_pred = pd.DataFrame(pred_prob, columns=label_map)\n",
    "print(df_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kesimpulan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest dengan representasi TF-IDF adalah model terbaik untuk sistem rekomendasi mentee-role WBL, dengan F1-Score 0.981. Model ini menunjukkan akurasi tinggi, konsistensi precision dan recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
