{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Word2Vec Embedding plus Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load file yang sudah disimpan\n",
    "file_path = \"*/transformed_src/df_lowercased.csv\"\n",
    "df_lowercased = pd.read_csv(file_path)\n",
    "\n",
    "# Gabungkan kolom-kolom menjadi satu kolom 'text'\n",
    "df_lowercased['text'] = df_lowercased[['spv_mentee', 'job_position', 'required_tools', \n",
    "                                       'required_skills', 'required_role_title', 'mentee_name', \n",
    "                                       'mentee_title', 'mentee_skill', 'mentee_tools', \n",
    "                                       'mentee_position', 'mentee_status']\n",
    "                                      ].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Buat dataframe akhir untuk modeling\n",
    "df_combined = df_lowercased[['text', 'mentee_status']].rename(columns={'mentee_status': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of the Dataset with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_combined['labels'] = encoder.fit_transform(df_combined['labels'])\n",
    "\n",
    "X = df_combined['text']\n",
    "y = df_combined['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Hapus semua path sebelumnya\n",
    "nltk.data.path.clear()\n",
    "\n",
    "# Tambahkan path yang benar\n",
    "nltk.data.path.append('*/nltk_data')\n",
    "\n",
    "# Verifikasi path yang digunakan\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.data.path.append('*/nltk_data')\n",
    "\n",
    "nltk.download('punkt', download_dir='*/nltk_data')\n",
    "nltk.download('stopwords', download_dir='*/nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='*/nltk_data')\n",
    "\n",
    "# Fungsi preprocessing tanpa stemming\n",
    "def preprocess(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Menghapus karakter non-alfabet\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Menurunkan semua huruf ke kecil\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tentukan bahasa berdasarkan kehadiran kata-kata bahasa Indonesia atau Inggris\n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    stop_words_id = set(stopwords.words('indonesian'))\n",
    "    \n",
    "    # Tentukan stopwords dan bahasa\n",
    "    words = nltk.word_tokenize(text)\n",
    "    language = 'id' if any(word in stop_words_id for word in words) else 'en'\n",
    "\n",
    "    if language == 'en':\n",
    "        stop_words = stop_words_en\n",
    "    else:\n",
    "        stop_words = stop_words_id\n",
    "    \n",
    "    # Tokenisasi dan hapus stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train)\n",
    "X_test = pd.Series(X_test)\n",
    "\n",
    "# Tidak usah isi NaN, biarkan tetap kosong\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "WINDOW = 50\n",
    "MIN_COUNT = 5\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, vector_size=VECTOR_SIZE, window=WINDOW, min_count=MIN_COUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    return np.mean(words_vecs, axis=0)\n",
    "\n",
    "X_train_vec = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test_vec = np.array([vectorize(sentence) for sentence in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengecek apakah ada nilai negatif di X_train_vec dan X_test_vec\n",
    "print(\"Cek nilai negatif di X_train_vec:\")\n",
    "print(f\"Ada nilai negatif di X_train_vec: {np.any(X_train_vec < 0)}\")\n",
    "print(f\"Jumlah nilai negatif di X_train_vec: {np.sum(X_train_vec < 0)}\")\n",
    "\n",
    "print(\"\\nCek nilai negatif di X_test_vec:\")\n",
    "print(f\"Ada nilai negatif di X_test_vec: {np.any(X_test_vec < 0)}\")\n",
    "print(f\"Jumlah nilai negatif di X_test_vec: {np.sum(X_test_vec < 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cek apakah ada nilai negatif atau positif di X_train_vec dan X_test_vec\n",
    "print(\"Cek nilai negatif dan positif di X_train_vec:\")\n",
    "\n",
    "# Mengecek nilai negatif\n",
    "negative_values_train = X_train_vec < 0\n",
    "positive_values_train = X_train_vec > 0\n",
    "\n",
    "# Menampilkan jumlah nilai negatif dan positif\n",
    "print(f\"Jumlah nilai negatif di X_train_vec: {np.sum(negative_values_train)}\")\n",
    "print(f\"Jumlah nilai positif di X_train_vec: {np.sum(positive_values_train)}\")\n",
    "\n",
    "# Cek X_test_vec\n",
    "print(\"\\nCek nilai negatif dan positif di X_test_vec:\")\n",
    "\n",
    "negative_values_test = X_test_vec < 0\n",
    "positive_values_test = X_test_vec > 0\n",
    "\n",
    "# Menampilkan jumlah nilai negatif dan positif\n",
    "print(f\"Jumlah nilai negatif di X_test_vec: {np.sum(negative_values_test)}\")\n",
    "print(f\"Jumlah nilai positif di X_test_vec: {np.sum(positive_values_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from time import time\n",
    "\n",
    "# Fungsi Benchmarking\n",
    "def benchmark(clf, X_train, y_train, X_test, y_test, custom_name=False):\n",
    "    print(\"_\" * 80)\n",
    "    print(f\"Training: {clf}\")\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3f}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3f}s\")\n",
    "\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, pred, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"accuracy:   {score:.3f}\")\n",
    "    print(f\"precision:  {precision:.3f}\")\n",
    "    print(f\"recall:     {recall:.3f}\")\n",
    "    print(f\"f1-score:   {f1:.3f}\")\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(custom_name) if custom_name else clf.__class__.__name__\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "# Daftar Model tanpa Naive Bayes dan Confusion Matrix\n",
    "models = [\n",
    "    (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\n",
    "    (RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\n",
    "    (KNeighborsClassifier(n_neighbors=5), \"k-Nearest Neighbors\"),\n",
    "    (RandomForestClassifier(n_estimators=100, random_state=42), \"Random Forest\"),\n",
    "    (LinearSVC(C=0.1, dual=False, max_iter=1000), \"Linear SVC\"),\n",
    "    (SGDClassifier(loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True), \"Log-loss SGD\"),\n",
    "    (NearestCentroid(), \"Nearest Centroid\")\n",
    "]\n",
    "\n",
    "# Evaluasi Semua Model\n",
    "print(\"=\" * 80)\n",
    "print(\"Benchmarking Model untuk Word2Vec\\n\")\n",
    "\n",
    "# Misalnya X_train_vec, X_test_vec adalah hasil representasi Word2Vec untuk data training dan testing\n",
    "results = []\n",
    "for clf, name in models:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Evaluasi: {name}\")\n",
    "    clf_descr, score, train_time, test_time = benchmark(clf, X_train_vec, y_train, X_test_vec, y_test, name)\n",
    "    results.append((clf_descr, score, train_time, test_time))\n",
    "\n",
    "# Jika ingin melihat hasilnya lebih lanjut\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Latih model Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Prediksi menggunakan X_test_vec\n",
    "y_pred_lr = clf.predict(X_test_vec)\n",
    "\n",
    "# Evaluasi model\n",
    "precision = precision_score(y_test, y_pred_lr, average='macro')\n",
    "recall = recall_score(y_test, y_pred_lr, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_lr, average='macro')\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print('Model Logistic Regression untuk Prediksi Status Mentee')\n",
    "print(f'Precision: {round(precision, 3)}')\n",
    "print(f'Recall: {round(recall, 3)}')\n",
    "print(f'F1-Score: {round(f1, 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_vec, y_train)\n",
    "y_pred_rf = rf.predict(X_test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi\n",
    "precision = precision_score(y_test, y_pred_rf, average='macro')\n",
    "recall = recall_score(y_test, y_pred_rf, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_rf, average='macro')\n",
    "\n",
    "print(f\"RandomForest / Precision: {round(precision,3)} / Recall: {round(recall,3)} / F1-Score: {round(f1,3)}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf, labels=rf.classes_)\n",
    "ax = sns.heatmap(cm, annot=True)\n",
    "ax.set_title('Confusion Matrix (Word2Vec + RandomForest)', fontsize=16)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Target\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# Daftar model\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(C=5, max_iter=1000),\n",
    "    \"Ridge Classifier\": RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Linear SVC\": LinearSVC(C=0.1, dual=False, max_iter=1000),\n",
    "    \"Log-loss SGD\": SGDClassifier(loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True),\n",
    "    \"Nearest Centroid\": NearestCentroid()\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_results = {}\n",
    "\n",
    "X_clean = X.apply(preprocess)\n",
    "X_w2v = np.array([vectorize(sentence) for sentence in X_clean])\n",
    "\n",
    "\n",
    "# Uji semua model\n",
    "for model_name, clf in models.items():\n",
    "    print(f\"\\nEvaluating: {model_name}\")\n",
    "    results = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": [],\n",
    "        \"train_time\": [],\n",
    "        \"test_time\": []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_w2v, y), 1):\n",
    "        X_train, X_test = X_w2v[train_idx], X_w2v[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        t0 = time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_duration = time() - t0\n",
    "\n",
    "        t0 = time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        test_duration = time() - t0\n",
    "\n",
    "        prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        results[\"precision\"].append(prec)\n",
    "        results[\"recall\"].append(rec)\n",
    "        results[\"f1_score\"].append(f1)\n",
    "        results[\"train_time\"].append(train_duration)\n",
    "        results[\"test_time\"].append(test_duration)\n",
    "\n",
    "        print(f\"  Fold {fold}: acc={acc:.3f}, f1={f1:.3f}\")\n",
    "\n",
    "    # Simpan rata-rata hasil\n",
    "    all_results[model_name] = {\n",
    "        \"Mean Precision\": np.mean(results[\"precision\"]),\n",
    "        \"Mean Recall\": np.mean(results[\"recall\"]),\n",
    "        \"Mean F1-Score\": np.mean(results[\"f1_score\"]),\n",
    "        \"Avg Train Time\": np.mean(results[\"train_time\"]),\n",
    "        \"Avg Test Time\": np.mean(results[\"test_time\"]),\n",
    "    }\n",
    "\n",
    "# Tampilkan hasil akhir dalam bentuk DataFrame\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Benchmarking Hasil Rata-Rata (Word2Vec + 10-Fold Cross-Validation):\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "print(results_df.round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
